{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"DSCI-272: Predicting with Cloudera Machine Learning Exercises Introduction to CML on CDP Streamlit on CML Data - Access, Audit, and Mask Experiment Tracking Visualize Duocar Data Workbench Lecture and Exercises Autoscaling, Performance, and GPU Settings Continuous Model Modeling with Evidently AI","title":"Home"},{"location":"index.html#dsci-272-predicting-with-cloudera-machine-learning","text":"","title":"DSCI-272: Predicting with Cloudera Machine Learning"},{"location":"index.html#exercises","text":"Introduction to CML on CDP Streamlit on CML Data - Access, Audit, and Mask Experiment Tracking Visualize Duocar Data Workbench Lecture and Exercises Autoscaling, Performance, and GPU Settings Continuous Model Modeling with Evidently AI","title":"Exercises"},{"location":"autoscaling/index.html","text":"Autoscaling, Performance, and GPU Settings The time-saving potential of using GPUs for complex and large tasks is massive, setting up these environments and tasks such as wrangling NVIDIA drivers, managing CUDA versions and deploying custom engines for your specific project needs can be time consuming and challenging. To make these processes simpler \u2014 and to get data scientists working on ML use cases faster \u2014 CML made it simple to configure and leverage NVIDIA GPUs natively. In this exercise, you will use a Computer Vision Image Classification example and train a deep learning model to classify fashion items leveraging the Fashion MNIST Dataset. The main focus of the exercise is to demonstrate how to use the GPU and auto scaling features of CML. If you would like to know more about the Computer Vision Image Classification example, you can view the project on Github . In this exercise, you will: Run an image classification without a GPU, Run and image classification example with a GPU, and Examine the difference in performance. View Workspace Details and Allocated GPUs Select Machine Learning from the Cloudera Data Platform home page. Select the three dot icon in the Actions column for your workspace. Click View Workspace Details . Scroll down to view Workspace Instances . In the example below, there are no CML GPU Workers as indicated by the zero in the Count program. The Autoscale Range indicates that there could be zero to ten instances. Your workspace may show different values. For example, if someone else already requested a session with a GPU, you may have a one or greater value in the Count column. In the example below, the Instance Type for the CML GPU Worker is an AWS p2.8xlarge . This is a fairly beefy instance with 8 GPU cores. Therefore, if one instance is running, it can support up to eight sessions with one GPU before another instance will be automatically allocated by the auto scaling feature. Leave the Workspace Details tab open in your browser. Create a New Project Open a new browser tab and navigate to your CML workspace. Click the New Project button. Enter Deep Learning with GPUs for the Name. Click Git and enter https://github.com/bshimel-cloudera/edu_cml_deeplearning_with_gpu for the URL. Select Python 3.7 as the Kernel . Check Add GPU enabled Runtime variant . This is extremely important. It tells CML to use the runtime with the GPU drivers. Click Create Project Create a Session without a GPU Click the New Session button. Enter Without GPU for the Session Name . Under Runtime , select Workbench , Nvidia GPU and verify Version is 2023.05 . Even though you will not be using a GPU in this example, you want the runtime with the Nvidia libraries for TensorFlow to work properly. Under Resource Profile , select 4 vCPU/8 GiB Memory and 0 GPUs . Click the Start Session button. Once the session has started, open the tensorflow/environment_setup.py file. Click the Run button to setup the environment. This step will take about seven minutes. Open a terminal window by clicking Terminal Access . Enter cd tensorflow to change the tensorflow directory. Then, enter time python main.py . This will run the image classification program and record the time to run the program. When the program finishes, note the real, user, and sys time required to run the program. Create a Session with a GPU Close the terminal and go back to the project. Create a new session titled With GPU . Select 4 vCPU/8 GiB Memory and 1 GPU . It might be tempting to select more than one GPU, but do not. The example is not configured to take advantage of more than one GPU. Click the Start Session button. It is hard to predict what will be seen in a shared environment. If a CML GPU Worker node is not available, you might see a message about Insufficient nvidia resources or pod taint . If you navigate back to Workspace Details in your other browser tab, you might see the CML GPU Worker node count still at zero. If this the case, the auto scaling will take over and start up a CML GPU Worker node. You will see a message like the following. Once the auto scaling does its magic, the Workspace Details will show that there is a new CML GPU Worker node. The whole process of auto scaling and adding a new worker node can take awhile (like ten minutes). Be patient, it is worth the wait. Once the new session has started, click Terminal Access to open a new terminal. Enter cd tensorflow , then enter time python main.py . Compare the new runtime using the GPU with your previous runtime without the GPU. The real runtime with a GPU should be about half of the real runtime without a GPU. Discuss the results with your instructor and classmates. Why is the user runtime so much greater for the non-GPU than the real runtime? End of Exercise","title":"Autoscaling, Performance, and GPU Settings"},{"location":"autoscaling/index.html#autoscaling-performance-and-gpu-settings","text":"The time-saving potential of using GPUs for complex and large tasks is massive, setting up these environments and tasks such as wrangling NVIDIA drivers, managing CUDA versions and deploying custom engines for your specific project needs can be time consuming and challenging. To make these processes simpler \u2014 and to get data scientists working on ML use cases faster \u2014 CML made it simple to configure and leverage NVIDIA GPUs natively. In this exercise, you will use a Computer Vision Image Classification example and train a deep learning model to classify fashion items leveraging the Fashion MNIST Dataset. The main focus of the exercise is to demonstrate how to use the GPU and auto scaling features of CML. If you would like to know more about the Computer Vision Image Classification example, you can view the project on Github . In this exercise, you will: Run an image classification without a GPU, Run and image classification example with a GPU, and Examine the difference in performance.","title":"Autoscaling, Performance, and GPU Settings"},{"location":"autoscaling/index.html#view-workspace-details-and-allocated-gpus","text":"Select Machine Learning from the Cloudera Data Platform home page. Select the three dot icon in the Actions column for your workspace. Click View Workspace Details . Scroll down to view Workspace Instances . In the example below, there are no CML GPU Workers as indicated by the zero in the Count program. The Autoscale Range indicates that there could be zero to ten instances. Your workspace may show different values. For example, if someone else already requested a session with a GPU, you may have a one or greater value in the Count column. In the example below, the Instance Type for the CML GPU Worker is an AWS p2.8xlarge . This is a fairly beefy instance with 8 GPU cores. Therefore, if one instance is running, it can support up to eight sessions with one GPU before another instance will be automatically allocated by the auto scaling feature. Leave the Workspace Details tab open in your browser.","title":"View Workspace Details and Allocated GPUs"},{"location":"autoscaling/index.html#create-a-new-project","text":"Open a new browser tab and navigate to your CML workspace. Click the New Project button. Enter Deep Learning with GPUs for the Name. Click Git and enter https://github.com/bshimel-cloudera/edu_cml_deeplearning_with_gpu for the URL. Select Python 3.7 as the Kernel . Check Add GPU enabled Runtime variant . This is extremely important. It tells CML to use the runtime with the GPU drivers. Click Create Project","title":"Create a New Project"},{"location":"autoscaling/index.html#create-a-session-without-a-gpu","text":"Click the New Session button. Enter Without GPU for the Session Name . Under Runtime , select Workbench , Nvidia GPU and verify Version is 2023.05 . Even though you will not be using a GPU in this example, you want the runtime with the Nvidia libraries for TensorFlow to work properly. Under Resource Profile , select 4 vCPU/8 GiB Memory and 0 GPUs . Click the Start Session button. Once the session has started, open the tensorflow/environment_setup.py file. Click the Run button to setup the environment. This step will take about seven minutes. Open a terminal window by clicking Terminal Access . Enter cd tensorflow to change the tensorflow directory. Then, enter time python main.py . This will run the image classification program and record the time to run the program. When the program finishes, note the real, user, and sys time required to run the program.","title":"Create a Session without a GPU"},{"location":"autoscaling/index.html#create-a-session-with-a-gpu","text":"Close the terminal and go back to the project. Create a new session titled With GPU . Select 4 vCPU/8 GiB Memory and 1 GPU . It might be tempting to select more than one GPU, but do not. The example is not configured to take advantage of more than one GPU. Click the Start Session button. It is hard to predict what will be seen in a shared environment. If a CML GPU Worker node is not available, you might see a message about Insufficient nvidia resources or pod taint . If you navigate back to Workspace Details in your other browser tab, you might see the CML GPU Worker node count still at zero. If this the case, the auto scaling will take over and start up a CML GPU Worker node. You will see a message like the following. Once the auto scaling does its magic, the Workspace Details will show that there is a new CML GPU Worker node. The whole process of auto scaling and adding a new worker node can take awhile (like ten minutes). Be patient, it is worth the wait. Once the new session has started, click Terminal Access to open a new terminal. Enter cd tensorflow , then enter time python main.py . Compare the new runtime using the GPU with your previous runtime without the GPU. The real runtime with a GPU should be about half of the real runtime without a GPU. Discuss the results with your instructor and classmates. Why is the user runtime so much greater for the non-GPU than the real runtime? End of Exercise","title":"Create a Session with a GPU"},{"location":"data_access/index.html","text":"Data - Access, Audit, and Mask This exercise uses the fictitious Duocar dataset. The data is stored in Amazon S3 and has been added to the company's data warehouse. Some of the data contains Personal Identifiable Information (PII). The company has requested the administrator only allow access to the information to those who need access to it to perform their job. In this instance, the full birth date is considered to be PII. Therefore, the birth date fields have been classified as PII and a PII policy has been created to mask the month and day. Note This exercise uses birth dates as PII. While a birth date may be considered PII in some scenarios, this exercise is completely hypothetical and has been designed to demonstrate the software and concepts. What data is PII and how to protect it is a legal issue and is beyond the scope of this course. Access Data Access the Data Warehouse by clicking Data Warehouse . Warning There is a known issue opening Hue with Safari on MacOS that results in an \"Invalid CORS request.\" Please use another browser, like Chrome or Firefox, if you experience this issue. Click Hue to launch Hue in a new browser tab. Once Hue launches, make sure you are in the Hive editor and duocar is the selected database. If you are not in the Hive editor, select the </> from the left side menu. If duocar is not selected, select duocar from the list of databases. If default selected and Tables is displayed, click the < to navigate back to the list of databases and then select duocar . Next, execute the following SQL by entering it into the editor and clicking the play/run button to show the drivers' data: select * from drivers limit 10 ; Notice that the year for each driver's birth date varies, but the day and month are always 01 . This is a result of the custom tag-based policy. Create a new joined table of birth dates. Now that you have seen the effects of masking on the original tables, what happens when you create a new table from tables that have been masked? In the following steps, you will create a new table from the drivers and riders tables. You will view the new table and its data. You will also view the table in the data catalog. Use the following SQL to create a new table that contains the driver's and rider's birth date for each ride. Replace the XX in the table name with your student number. create table birth_dates_XX as select riders . birth_date as rider_bd , drivers . birth_date as driver_bd from rides join riders on rides . rider_id = riders . id join drivers on rides . driver_id = drivers . id ; You may have to wait for Tez session. If so, it usually takes a few minutes for a session to start. Once the table is created, use a SELECT statement to view the new table's contents. Return the Data Warehouse tab in your browser and click the main menu icon. Click Data Catalog . Select the new table, birth_dates_XX where XX is your student number, from the list. View the new table's lineage. Select the Schema tab. What are the classifications for the new fields? End of Exercise","title":"Data - Access, Audit, and Mask"},{"location":"data_access/index.html#data-access-audit-and-mask","text":"This exercise uses the fictitious Duocar dataset. The data is stored in Amazon S3 and has been added to the company's data warehouse. Some of the data contains Personal Identifiable Information (PII). The company has requested the administrator only allow access to the information to those who need access to it to perform their job. In this instance, the full birth date is considered to be PII. Therefore, the birth date fields have been classified as PII and a PII policy has been created to mask the month and day. Note This exercise uses birth dates as PII. While a birth date may be considered PII in some scenarios, this exercise is completely hypothetical and has been designed to demonstrate the software and concepts. What data is PII and how to protect it is a legal issue and is beyond the scope of this course.","title":"Data - Access, Audit, and Mask"},{"location":"data_access/index.html#access-data","text":"Access the Data Warehouse by clicking Data Warehouse . Warning There is a known issue opening Hue with Safari on MacOS that results in an \"Invalid CORS request.\" Please use another browser, like Chrome or Firefox, if you experience this issue. Click Hue to launch Hue in a new browser tab. Once Hue launches, make sure you are in the Hive editor and duocar is the selected database. If you are not in the Hive editor, select the </> from the left side menu. If duocar is not selected, select duocar from the list of databases. If default selected and Tables is displayed, click the < to navigate back to the list of databases and then select duocar . Next, execute the following SQL by entering it into the editor and clicking the play/run button to show the drivers' data: select * from drivers limit 10 ; Notice that the year for each driver's birth date varies, but the day and month are always 01 . This is a result of the custom tag-based policy.","title":"Access Data"},{"location":"data_access/optional_instructor.html","text":"Protected Information Setup - Instructor Demo Birth dates, especially the full birth date, have been determined to be PII. The administrator (instructor) has been asked to find and hide all birth dates within the dataset. Apply PII Classification Click Data Catalog . Search for all of the birth date fields. Click in the search box and enter *birth* . Four fields with birth in the name are displayed. Click rider_birth_date in the list of items. The details for the rider_birth_date are displayed. Click Add Classification . Click in the Search and Add field. Enter PII and click Create . Enter PII for Name . Enter Personally Identifiable Information for Description . Click Create . The new PII classification is created. However, it will not show as a choice yet. Return to the search results by clicking Search in the left side menu. Then, click rider_birth_date . Click Add Classification and search for PII . Click the plus symbol next to PII . Click Save . Return to the search results and repeat the process to add the PII classification the the remaining three birth date fields. Apply Ranger Policies and Masking Once the fields have been tagged as PII. Ranger is used to limit access or mask the fields. Return to the Data Catalog Search page. Click the Ranger button. Select Tag Based Policies from the Access Manager menu. Click the cm_tag link. Select the Masking tab. Click the Add New Policy button. Enter Mask PII for the Policy Name . Select PII for the TAG . Under Mask Conditions , select public for Select Group . Click Add Permissions under Add Access Types**. Select hive as the component. Click the Hive checkbox. Click the checkmark button. Click Add Mask Type under Select Masking Options . Select Custom . Click the checkmark button. Enter cast(mask(cast(to_date({col}) as date), 'x', 'x', 'x', -1, '1', 1, 0, -1) as timestamp) into the custom mask field. Note You may wonder why Custom was chosen instead of Date show only year . The later only works with date fields. The birth date fields in the duocar database are timestamp fields. Had Data show only been chosen, the fields would have been set to Null, since the date value returned by the mask would not have matched the timestamp data type. Therefore, a custom mask is used to cast the result to timestamp. Click Add to create the new policy. The previous tag-based policy is sufficient to mask all of the date fields. The custom mask will return the year. The day and month will be set to 01 . However, an additional resource policy will be created to demonstrate how resource policies work, how policies interact, and the potential effect of a policy on propagated classifications. The following steps create a new resource-based Ranger policy that specifically masks the riders table birth_date field by returning NULL. Select Resource Based Policies from the Access Manager menu. Click the Hadoop SQL link. Select the Masking tab. Click the Add New Policy button. Enter Mask Rider Birth Date for the Policy Name . Select duocar as the Hive Database . Select riders as the Hive Table . Select birth_date as the Hive Column . Click Add Permissions under Access Types . Check select and click the checkmark button. Click the Select Masking Option link under Select Masking Options . Select Nullify and click the checkmark button. Select public under Select Group . Click Add .","title":"Optional instructor"},{"location":"data_access/optional_instructor.html#protected-information-setup-instructor-demo","text":"Birth dates, especially the full birth date, have been determined to be PII. The administrator (instructor) has been asked to find and hide all birth dates within the dataset.","title":"Protected Information Setup - Instructor Demo"},{"location":"data_access/optional_instructor.html#apply-ranger-policies-and-masking","text":"Once the fields have been tagged as PII. Ranger is used to limit access or mask the fields. Return to the Data Catalog Search page. Click the Ranger button. Select Tag Based Policies from the Access Manager menu. Click the cm_tag link. Select the Masking tab. Click the Add New Policy button. Enter Mask PII for the Policy Name . Select PII for the TAG . Under Mask Conditions , select public for Select Group . Click Add Permissions under Add Access Types**. Select hive as the component. Click the Hive checkbox. Click the checkmark button. Click Add Mask Type under Select Masking Options . Select Custom . Click the checkmark button. Enter cast(mask(cast(to_date({col}) as date), 'x', 'x', 'x', -1, '1', 1, 0, -1) as timestamp) into the custom mask field. Note You may wonder why Custom was chosen instead of Date show only year . The later only works with date fields. The birth date fields in the duocar database are timestamp fields. Had Data show only been chosen, the fields would have been set to Null, since the date value returned by the mask would not have matched the timestamp data type. Therefore, a custom mask is used to cast the result to timestamp. Click Add to create the new policy. The previous tag-based policy is sufficient to mask all of the date fields. The custom mask will return the year. The day and month will be set to 01 . However, an additional resource policy will be created to demonstrate how resource policies work, how policies interact, and the potential effect of a policy on propagated classifications. The following steps create a new resource-based Ranger policy that specifically masks the riders table birth_date field by returning NULL. Select Resource Based Policies from the Access Manager menu. Click the Hadoop SQL link. Select the Masking tab. Click the Add New Policy button. Enter Mask Rider Birth Date for the Policy Name . Select duocar as the Hive Database . Select riders as the Hive Table . Select birth_date as the Hive Column . Click Add Permissions under Access Types . Check select and click the checkmark button. Click the Select Masking Option link under Select Masking Options . Select Nullify and click the checkmark button. Select public under Select Group . Click Add .","title":"Apply Ranger Policies and Masking"},{"location":"data_visualization/index.html","text":"Visualize Duocar Data The Data Visualization application is good for exploring your data and sharing with others. In this exercise, you will: use the Data Visualization application to make a connection to the data warehouse, and create a dashboard to explore the ride data. Open your CML workspace and click New Project . Note It is not necessary to create a new project to use Data Visualization. In this case, each student is creating their own project in order to demonstrate some features which are project-wide. Enter Duocar X for the project name, where X is your student number. Click Create Project . Click Data in the project menu. Data Discovery and Visualization is just an application. The Data link in the project menu is just a convenient shortcut to launch and access the application. Click Launch Data Application to start the application. The Data application homepage has useful information to get started and shortcut to common and recently used items. Click Datasets in the menu at the top of the application.l The Data application always contains a samples dataset. Additional datasets are inherited from the project and CML workspace. Click the data warehouse dataset. If this is your first time accessing a dataset, you will receive an error that Data application cannot read a list of databases. This is because the Data application needs a Workload Password to access the data warehouse. Create Workload Password (if needed) If you received an error on the prior step, continue to follow the instructions below. If you did not receive an error, skip ahead to create a new dataset . Click the Main Menu in the upper left corner. Click Home . Click on your username in the lower right corner. Click Profile . Click Set Workload Password . Enter your workload password and click Set Workload Password . Click ML Workspaces in the left menu. Click your workspace. Click User Settings in the workspace menu on the left. Click the Environment Variables tab. Enter the workload password you just created and click Save . Now that your workload password has been set. The Data Discovery and Visualization application needs to be restarted. Click Projects . Click the Duocar X project, where X is your student number. Click Applications . Click \u22ee menu in the upper right corner of the Data Discovery and Visualization application. Click Restart Application . Click OK to confirm. Click the Data Discovery and Visualization application. Click Datasets . Click on your data warehouse connection. Create a New Dataset Click New Dataset Enter Duocar Joined for the Dataset Title . Select duocar as the database. Select joined as the table. Click Create . Click the newly created dataset, Duocar Joined . Click Data Model in the Dataset Detail menu . Click Show Data . This is a quick test to verify the dataset is working and has data. Create a Dashboard Click Visuals in the menu at the top of the application. Click the New Dashboard button. Enter Ride Dashboard for the title. Click Visuals in the DASH. menu on the right. Click the Bar Chart icon. The bar chart properties are displayed. Drag service from the list of Dimensions and drop it in the X Axis field. Drag Record Count from the list of Measures and drop it in the Y Axis field. Click the Refresh Visual button. Note The first time you make a connection to the data, the data warehouse may need start. This will cause the visual to take a long time to update as it is waiting for the data. The next five steps check the status of the data warehouse and watch as it auto provisions pods and enters a running state. If your visual updates quickly, you can skip these steps. Click Main Menu . Right-click on Data Warehouse . Click Open in New Tab to open a new browser tab. If your data warehouse was previously suspended, it shows Starting as it is provisioning resources. Once the data warehouse has provisioned its resources, the state will change to Running . Close the browser tab and return to the tab with the Data Discovery and Visualization application. Enter Rides by Service as the title for the bar chart visual. Click Visuals in the DASH. menu on the right. Click the New Visual button. Click the Pie icon. The pie chart properties are displayed. Drag star_rating from the list of Measures and drop it in the Dimension field. Drag Record Count from the list of Measures and drop it in the Measure field. Enter Star Rating as the title for the pie chart visual. Click Visuals in the DASH. menu on the right. Click the New Visual button. Click the Packed Bubbles icon. The packed bubbles properties are displayed. Drag rider_student from the list of Dimensions and drop it in the Dimension field. Drag Record Count from the list of Measures and drop it in the Measure field. Enter Student Riders as the title for the packed bubbles visual. Click Visuals in the DASH. menu on the right. Click the New Visual button. Click the Calendar Heatmap icon. The calendar heatmap properties are displayed. Drag date_time from the list of Dimensions and drop it in the Date field. Drag Record Count from the list of Measures and drop it in the Measure field. Enter Ride Heatmap by Date as the title for the calendar heatmap visual Click Refresh Visual . Notice, there is an \u24d8 icon displayed on the visual and only February has rides. Click the \u24d8 icon. The default limit for the number of rows fetched is 5,000. This is limiting the rides displayed to February. Click Settings in the right menu. Expand the Data category in the Visual Settings . Enter 50000 for the Maximum number of rows to fetch . The heatmap will show data for February through April. Create Filters Displaying the data is interesting but it is static. In order to explore the data and find the answers to questions like, \"What service do students prefer?\", filters can be add to the dashboard to make it interactive. Click Filters in the right menu. Click distance in the list of Measures . Notice, a distance filter is applied to the top of the dashboard. Click rider_student in the list of Dimensions . Click Save to save the dashboard. Click View to view the dashboard. Adjust distance sliders. Do students take longer or shorter rides in general? Reset the distance sliders. Use the filter to select only student riders. What day of the week has the most student riders in general? Share the Dashboard Click the \u22ef menu in the upper right corner of the dashboard. Click Get URL . The URL is displayed. The URL can be used to share the dashboard with other users. End of Exercise","title":"Visualize Duocar Data"},{"location":"data_visualization/index.html#visualize-duocar-data","text":"The Data Visualization application is good for exploring your data and sharing with others. In this exercise, you will: use the Data Visualization application to make a connection to the data warehouse, and create a dashboard to explore the ride data. Open your CML workspace and click New Project . Note It is not necessary to create a new project to use Data Visualization. In this case, each student is creating their own project in order to demonstrate some features which are project-wide. Enter Duocar X for the project name, where X is your student number. Click Create Project . Click Data in the project menu. Data Discovery and Visualization is just an application. The Data link in the project menu is just a convenient shortcut to launch and access the application. Click Launch Data Application to start the application. The Data application homepage has useful information to get started and shortcut to common and recently used items. Click Datasets in the menu at the top of the application.l The Data application always contains a samples dataset. Additional datasets are inherited from the project and CML workspace. Click the data warehouse dataset. If this is your first time accessing a dataset, you will receive an error that Data application cannot read a list of databases. This is because the Data application needs a Workload Password to access the data warehouse.","title":"Visualize Duocar Data"},{"location":"data_visualization/index.html#create-workload-password-if-needed","text":"If you received an error on the prior step, continue to follow the instructions below. If you did not receive an error, skip ahead to create a new dataset . Click the Main Menu in the upper left corner. Click Home . Click on your username in the lower right corner. Click Profile . Click Set Workload Password . Enter your workload password and click Set Workload Password . Click ML Workspaces in the left menu. Click your workspace. Click User Settings in the workspace menu on the left. Click the Environment Variables tab. Enter the workload password you just created and click Save . Now that your workload password has been set. The Data Discovery and Visualization application needs to be restarted. Click Projects . Click the Duocar X project, where X is your student number. Click Applications . Click \u22ee menu in the upper right corner of the Data Discovery and Visualization application. Click Restart Application . Click OK to confirm. Click the Data Discovery and Visualization application. Click Datasets . Click on your data warehouse connection.","title":"Create Workload Password (if needed)"},{"location":"data_visualization/index.html#create-a-new-dataset","text":"Click New Dataset Enter Duocar Joined for the Dataset Title . Select duocar as the database. Select joined as the table. Click Create . Click the newly created dataset, Duocar Joined . Click Data Model in the Dataset Detail menu . Click Show Data . This is a quick test to verify the dataset is working and has data.","title":"Create a New Dataset"},{"location":"data_visualization/index.html#create-a-dashboard","text":"Click Visuals in the menu at the top of the application. Click the New Dashboard button. Enter Ride Dashboard for the title. Click Visuals in the DASH. menu on the right. Click the Bar Chart icon. The bar chart properties are displayed. Drag service from the list of Dimensions and drop it in the X Axis field. Drag Record Count from the list of Measures and drop it in the Y Axis field. Click the Refresh Visual button. Note The first time you make a connection to the data, the data warehouse may need start. This will cause the visual to take a long time to update as it is waiting for the data. The next five steps check the status of the data warehouse and watch as it auto provisions pods and enters a running state. If your visual updates quickly, you can skip these steps. Click Main Menu . Right-click on Data Warehouse . Click Open in New Tab to open a new browser tab. If your data warehouse was previously suspended, it shows Starting as it is provisioning resources. Once the data warehouse has provisioned its resources, the state will change to Running . Close the browser tab and return to the tab with the Data Discovery and Visualization application. Enter Rides by Service as the title for the bar chart visual. Click Visuals in the DASH. menu on the right. Click the New Visual button. Click the Pie icon. The pie chart properties are displayed. Drag star_rating from the list of Measures and drop it in the Dimension field. Drag Record Count from the list of Measures and drop it in the Measure field. Enter Star Rating as the title for the pie chart visual. Click Visuals in the DASH. menu on the right. Click the New Visual button. Click the Packed Bubbles icon. The packed bubbles properties are displayed. Drag rider_student from the list of Dimensions and drop it in the Dimension field. Drag Record Count from the list of Measures and drop it in the Measure field. Enter Student Riders as the title for the packed bubbles visual. Click Visuals in the DASH. menu on the right. Click the New Visual button. Click the Calendar Heatmap icon. The calendar heatmap properties are displayed. Drag date_time from the list of Dimensions and drop it in the Date field. Drag Record Count from the list of Measures and drop it in the Measure field. Enter Ride Heatmap by Date as the title for the calendar heatmap visual Click Refresh Visual . Notice, there is an \u24d8 icon displayed on the visual and only February has rides. Click the \u24d8 icon. The default limit for the number of rows fetched is 5,000. This is limiting the rides displayed to February. Click Settings in the right menu. Expand the Data category in the Visual Settings . Enter 50000 for the Maximum number of rows to fetch . The heatmap will show data for February through April.","title":"Create a Dashboard"},{"location":"data_visualization/index.html#create-filters","text":"Displaying the data is interesting but it is static. In order to explore the data and find the answers to questions like, \"What service do students prefer?\", filters can be add to the dashboard to make it interactive. Click Filters in the right menu. Click distance in the list of Measures . Notice, a distance filter is applied to the top of the dashboard. Click rider_student in the list of Dimensions . Click Save to save the dashboard. Click View to view the dashboard. Adjust distance sliders. Do students take longer or shorter rides in general? Reset the distance sliders. Use the filter to select only student riders. What day of the week has the most student riders in general?","title":"Create Filters"},{"location":"data_visualization/index.html#share-the-dashboard","text":"Click the \u22ef menu in the upper right corner of the dashboard. Click Get URL . The URL is displayed. The URL can be used to share the dashboard with other users. End of Exercise","title":"Share the Dashboard"},{"location":"example_exercise/index.html","text":"Creating a Streaming Application with a Kafka Source This is just an example of an exercise for reference purposes. In this exercise, you will use Flink to read Avro data from a Kafka source. In the second half of the exercise, you will use Flink to transform the data and stream the maximum driver distance to Kafka. Generate Kafka Data Stream Read Kafka Data Stream Write Driver's Maximum Distance to Kafka Generate Kafka Data Stream In this portion of the exercise, you will use a pre-built data generator to create a Kafka data stream. The data generator writes a stream of ride data to Kafka in the Avro data format. The schema for the data is stored in the Schema Registry. You will use the Streams Messaging Manager and Schema Registry to view the data and schema, respectively. Open a terminal and navigate to the data/generator directory. $ cd ~/training_materials/flink/data/generator Build the data generator with the following Maven command: $ mvn clean package Install the generator library file in your local Maven repository. $ mvn install Note: This step is required because we are using Avro data. The following Flink job will depend on this library for the Ride class generated by Avro. Run the data generator with the following commands: $ scripts/generate-rides.sh Leave the terminal window open and the generator running. Open Firefox. Chose the STREAMS MESSAGING MANAGER bookmark. Click the profile icon for the duocar-rides topic. View the duocar-rides metrics. Note, the metrics displayed are for the selected time interval as specified in the drop-down selection on the top right of the metrics page. Since this is a new stream, the metrics will likely be zero until the alloted time has passed. Select Data Explorer . (1) Select Integer for the Keys Deserializer. (2) Select Avro for the Values Deserializer. You will see the Keys and Values in the table below change from binary to human-readable. The schema for the Avro data will also be displayed. Click the Schema Registry icon to open the Schema Registry. Click the left-pointing arrow next to duocar-rides to view more information about the schema. You may close the Schema Registry tab in Firefox when you are done. Read Kafka Data Stream with Flink In this exercise, you will run a Flink job to read a stream of rides from Kafka. First, you will run the job locally. Then, you will run the job on the cluster. Open IntelliJ. Open ~/training_materials/flink/kafka as a new project. Navigate to KafkaJob in the src directory and open it in the editor. Examine the main method of the KafkaJob . Run the KafkaJob in IntelliJ and view the output. Stop KafkaJob . Open a terminal. Navigate to the Kafka exercise directory. $ cd /home/training/training_materials/flink/kafka Build and package KafkaJob. $ mvn clean package Run the job on the cluster. $ flink run \\ --detached \\ --jobmanager yarn-cluster \\ --yarnname KafkaJob \\ --class com.training.flink.KafkaJob \\ target/kafka-1.0-SNAPSHOT.jar View the job in the Flink Dashboard. Verify the messages are being sent to standard output. Cancel the job using the Flink Dashboard. Write Driver's Maximum Distance to Kafka In this exercise, you will transform the stream of ride data to a stream of the maximum distance for each driver that will be dynamically updated as new rides are processed. You will create a new Kafka producer and use the transformations you learned in the streaming exercise to create a new stream of strings with each drivers current maximum distance in the following format: { \"driver\" : 12345 , \"maxDistance\" : 999999 } Follow the steps below to modify KafkaJob to produce the desired output. Run the job locally until you have it working. Once the job works, submit it to the cluster. Note: The solution for the following steps is located in the solution folder next to the src folder in IntelliJ or, specifically, ~/training_materials/flink/kafka/solution/main/java/com/training/flink/KafkaJob.java for your reference. You cannot run the solution directly from the solution folder, but you can copy/paste the code or file into the src to execute it. Add a FlinkKafkaProducer to write the new string to a Kafka topic named duocar-driver-max-distance . Hint, consider SimpleStringSchema for your schema. Group your ride stream data by driver. Find the maximum distance. Convert the stream of POJO ride data to a string of driver ID, maxDistance as shown above. Add the Kafka producer as a sink. Hint, replace .print() . Once you have completed your code and submitted it to the cluster, view the results in STREAMS MESSAGING MANAGER and the Flink Dashboard . Cancel the job. Return the data generation script we started at the beginning of the lab and stop it. This is the end of the exercise.","title":"Creating a Streaming Application with a Kafka Source"},{"location":"example_exercise/index.html#creating-a-streaming-application-with-a-kafka-source","text":"","title":"Creating a Streaming Application with a Kafka Source"},{"location":"example_exercise/index.html#generate-kafka-data-stream","text":"In this portion of the exercise, you will use a pre-built data generator to create a Kafka data stream. The data generator writes a stream of ride data to Kafka in the Avro data format. The schema for the data is stored in the Schema Registry. You will use the Streams Messaging Manager and Schema Registry to view the data and schema, respectively. Open a terminal and navigate to the data/generator directory. $ cd ~/training_materials/flink/data/generator Build the data generator with the following Maven command: $ mvn clean package Install the generator library file in your local Maven repository. $ mvn install Note: This step is required because we are using Avro data. The following Flink job will depend on this library for the Ride class generated by Avro. Run the data generator with the following commands: $ scripts/generate-rides.sh Leave the terminal window open and the generator running. Open Firefox. Chose the STREAMS MESSAGING MANAGER bookmark. Click the profile icon for the duocar-rides topic. View the duocar-rides metrics. Note, the metrics displayed are for the selected time interval as specified in the drop-down selection on the top right of the metrics page. Since this is a new stream, the metrics will likely be zero until the alloted time has passed. Select Data Explorer . (1) Select Integer for the Keys Deserializer. (2) Select Avro for the Values Deserializer. You will see the Keys and Values in the table below change from binary to human-readable. The schema for the Avro data will also be displayed. Click the Schema Registry icon to open the Schema Registry. Click the left-pointing arrow next to duocar-rides to view more information about the schema. You may close the Schema Registry tab in Firefox when you are done.","title":"Generate Kafka Data Stream"},{"location":"example_exercise/index.html#read-kafka-data-stream-with-flink","text":"In this exercise, you will run a Flink job to read a stream of rides from Kafka. First, you will run the job locally. Then, you will run the job on the cluster. Open IntelliJ. Open ~/training_materials/flink/kafka as a new project. Navigate to KafkaJob in the src directory and open it in the editor. Examine the main method of the KafkaJob . Run the KafkaJob in IntelliJ and view the output. Stop KafkaJob . Open a terminal. Navigate to the Kafka exercise directory. $ cd /home/training/training_materials/flink/kafka Build and package KafkaJob. $ mvn clean package Run the job on the cluster. $ flink run \\ --detached \\ --jobmanager yarn-cluster \\ --yarnname KafkaJob \\ --class com.training.flink.KafkaJob \\ target/kafka-1.0-SNAPSHOT.jar View the job in the Flink Dashboard. Verify the messages are being sent to standard output. Cancel the job using the Flink Dashboard.","title":"Read Kafka Data Stream with Flink"},{"location":"example_exercise/index.html#write-drivers-maximum-distance-to-kafka","text":"In this exercise, you will transform the stream of ride data to a stream of the maximum distance for each driver that will be dynamically updated as new rides are processed. You will create a new Kafka producer and use the transformations you learned in the streaming exercise to create a new stream of strings with each drivers current maximum distance in the following format: { \"driver\" : 12345 , \"maxDistance\" : 999999 } Follow the steps below to modify KafkaJob to produce the desired output. Run the job locally until you have it working. Once the job works, submit it to the cluster. Note: The solution for the following steps is located in the solution folder next to the src folder in IntelliJ or, specifically, ~/training_materials/flink/kafka/solution/main/java/com/training/flink/KafkaJob.java for your reference. You cannot run the solution directly from the solution folder, but you can copy/paste the code or file into the src to execute it. Add a FlinkKafkaProducer to write the new string to a Kafka topic named duocar-driver-max-distance . Hint, consider SimpleStringSchema for your schema. Group your ride stream data by driver. Find the maximum distance. Convert the stream of POJO ride data to a string of driver ID, maxDistance as shown above. Add the Kafka producer as a sink. Hint, replace .print() . Once you have completed your code and submitted it to the cluster, view the results in STREAMS MESSAGING MANAGER and the Flink Dashboard . Cancel the job. Return the data generation script we started at the beginning of the lab and stop it. This is the end of the exercise.","title":"Write Driver's Maximum Distance to Kafka"},{"location":"experiment_tracking/index.html","text":"Experiment Tracking Tracking experiments is critical for knowing what parameters were used to generate a model and how the model is performing. CML projects provide experiment tracking based on MLflow to make this task easier. In this exercise, you will: use a Git repository to create your project, use the CML experiments feature to track a experiment, and commit the changes back to git. Create a New Project from Github Navigate to your workspace. Click the New Project button. Enter Experiments - Student # for the project name. Click Git under Initial Setup . Enter the following for the Git URL : https://github.com/bshimel-cloudera/edu-cml-on-cdp-experiments Select Python 3.9 for the Kernel . Click Create Project . CML will now checkout the content from Github and add it to the project. View Code and Run Job The next step is to look at the code and then run it. Since this exercise is focused on the use of Experiments feature, the code is very simple. There is a single file, add.py . It takes a series of numbers and calculates the sum. In this example, the numbers passed as arguments to the add.py program are acting as our parameters, the sum() function is acting as our machine learning model, and the total is serving as the prediction or model output. Obviously, the \"model\" in this case will be 100% accurate, but it will allow the exploration of the Experiment tracking features of CML. View the file by selecting the code folder from the list of files. Click on the python folder, and then click on the add.py file. You can see that it is a very simple, three-line program. Click Jobs in the left-side menu. Click New Job . Name the new job Add It Up . Click the Browse folder. Select the add.py file. The program accepts a list of numbers, separated by spaces. Enter something like 20 30 for the Arguments . Click Create Job . The new job is created, but not executed. Click Run as me to execute the job. Once the Status has changed to Success , click on the job name. The job overview is displayed. Select the History tab. A list of job runs is displayed. There should only be one job run at this point. Click on the run. View the session output. The sum of the arguments is displayed. Creating an Experiment Click Files in the left-side menu. Navigate to add.py and click Open in Session . The file is opened in the Workspace file editor. An actual session is not needed to edit the file. Edit the file to look as follows: import sys import mlflow mlflow . set_experiment ( \"Add It Up\" ) mlflow . start_run () params = [ int ( number ) for number in sys . argv [ 1 :]] total = sum ( params ) mlflow . log_param ( \"Input\" , params ) mlflow . log_metric ( \"Sum\" , total ) print ( f \"The sum of the numbers is: { total } \" ) mlflow . end_run () Here is a brief description of the added lines: import mlflow - This line imports the mlflow module. mlflow.start_run() - MLflow can track multiple runs. In this case, only one run is tracked per execution. This line marks the start of the run. mlflow.set_experiment(\"Add It Up\") - This line specifies which experiment to use. If the experiment does not exist, it will be created. mlflow.log_param(\"Input\", params) - This line tracks the input parameters. mlflow.log_metric(\"Sum\", total) - This line tracks the output metrics. mlflow.end_run() - This line marks the end of the run. Click File and then Save . Click Project to return to the project. Click Jobs in the left-side menu. Click Run as me to execute the job again. Once the Status changes to Success , click on the job name. Click on the History tab. Click on the latest run. In the output, a message is displayed to notify you that a new experiment was created. Click Experiments in the left-side menu. Click on the newly created experiment. Information about the experiment and a list of runs is displayed. The parameters and metrics for the run are displayed. Click on the run. A more detailed view of the run is displayed. Notice that in addition to the parameters and metrics, a run can have more information like notes, tags, and artifacts. Click Files in the left-side menu. Navigate to add.py and click Open in Session . Add the following code to create a new metric called count and a new artifact, the result.txt file: import sys import mlflow mlflow . set_experiment ( \"Add It Up\" ) mlflow . start_run () params = [ int ( number ) for number in sys . argv [ 1 :]] count = len ( params ) total = sum ( params ) with open ( \"result.txt\" , \"w\" ) as output_file : output_file . write ( f \"Input: { params } ,\" ) output_file . write ( f \"Count: { count } ,\" ) output_file . write ( f \"Sum: { total } \\n \" ) output_file . close () mlflow . log_param ( \"Input\" , params ) mlflow . log_metric ( \"Count\" , count ) mlflow . log_metric ( \"Sum\" , total ) mlflow . log_artifact ( \"result.txt\" ) print ( f \"The sum of the numbers is: { total } \" ) mlflow . end_run () Here is a brief description of the added lines: * count = len(params) - This is a new metric for tracking the number of numbers to sum. with open(\"result.txt\", \"w\") ... - This line and the following four lines create a file (artifact) with the results of the run. mlflow.log_metric(\"Count\", count) - This line tracks the new count metric. mlflow.log_artifact(\"result.txt\") - This line tracks the new artifact. Save the file and click Project to return to the project. Click Jobs in the left-side menu and click Run as me on the job. Click Experiments in the left-side menu and click the experiment in the list. Notice, the list of runs includes the new Count metric. Click on the latest run. The run details includes the new metric too. Scroll down, if needed, and view the Artifacts. If the job recently completed, the Artifacts may not have updated. Wait a minute and refresh the page, if you do not see the result.txt file in the list of Artifacts. Change the Input and Compare Runs Click Jobs in the left-side menu and click the Add It Up job in the list. Click the Settings tab. Edit the Arguments to be something different, like 20 30 40 . Scroll down and click Update Job Click Run as me . Click Experiments in the left-side menu and click on the Add It Up experiment. Select last two runs by clicking the respective checkboxes. Click Compare . Compare the results of the two runs using the table at the top and the various plots. Commit the Changes to Git Now that you have modified the add.py code to track metrics, you will commit the changes to git so they won't be lost. Click Sessions in the left-side menu and click New Session . Enter a session name and click Start Session . Once the session has started, click the Terminal Access button. Enter git status and press Enter . You will see add.py has been modified. Add the file to git and commit the changes. git add code/python/add.py git commit -m \"Added tracking to add.py\" Your file changes have been committed. Normally, you would push the changes back to a remote repository. However, in this case, you do not have permission to commit to the remote repository. End of Exercise","title":"Experiment Tracking"},{"location":"experiment_tracking/index.html#experiment-tracking","text":"Tracking experiments is critical for knowing what parameters were used to generate a model and how the model is performing. CML projects provide experiment tracking based on MLflow to make this task easier. In this exercise, you will: use a Git repository to create your project, use the CML experiments feature to track a experiment, and commit the changes back to git.","title":"Experiment Tracking"},{"location":"experiment_tracking/index.html#create-a-new-project-from-github","text":"Navigate to your workspace. Click the New Project button. Enter Experiments - Student # for the project name. Click Git under Initial Setup . Enter the following for the Git URL : https://github.com/bshimel-cloudera/edu-cml-on-cdp-experiments Select Python 3.9 for the Kernel . Click Create Project . CML will now checkout the content from Github and add it to the project.","title":"Create a New Project from Github"},{"location":"experiment_tracking/index.html#view-code-and-run-job","text":"The next step is to look at the code and then run it. Since this exercise is focused on the use of Experiments feature, the code is very simple. There is a single file, add.py . It takes a series of numbers and calculates the sum. In this example, the numbers passed as arguments to the add.py program are acting as our parameters, the sum() function is acting as our machine learning model, and the total is serving as the prediction or model output. Obviously, the \"model\" in this case will be 100% accurate, but it will allow the exploration of the Experiment tracking features of CML. View the file by selecting the code folder from the list of files. Click on the python folder, and then click on the add.py file. You can see that it is a very simple, three-line program. Click Jobs in the left-side menu. Click New Job . Name the new job Add It Up . Click the Browse folder. Select the add.py file. The program accepts a list of numbers, separated by spaces. Enter something like 20 30 for the Arguments . Click Create Job . The new job is created, but not executed. Click Run as me to execute the job. Once the Status has changed to Success , click on the job name. The job overview is displayed. Select the History tab. A list of job runs is displayed. There should only be one job run at this point. Click on the run. View the session output. The sum of the arguments is displayed.","title":"View Code and Run Job"},{"location":"experiment_tracking/index.html#creating-an-experiment","text":"Click Files in the left-side menu. Navigate to add.py and click Open in Session . The file is opened in the Workspace file editor. An actual session is not needed to edit the file. Edit the file to look as follows: import sys import mlflow mlflow . set_experiment ( \"Add It Up\" ) mlflow . start_run () params = [ int ( number ) for number in sys . argv [ 1 :]] total = sum ( params ) mlflow . log_param ( \"Input\" , params ) mlflow . log_metric ( \"Sum\" , total ) print ( f \"The sum of the numbers is: { total } \" ) mlflow . end_run () Here is a brief description of the added lines: import mlflow - This line imports the mlflow module. mlflow.start_run() - MLflow can track multiple runs. In this case, only one run is tracked per execution. This line marks the start of the run. mlflow.set_experiment(\"Add It Up\") - This line specifies which experiment to use. If the experiment does not exist, it will be created. mlflow.log_param(\"Input\", params) - This line tracks the input parameters. mlflow.log_metric(\"Sum\", total) - This line tracks the output metrics. mlflow.end_run() - This line marks the end of the run. Click File and then Save . Click Project to return to the project. Click Jobs in the left-side menu. Click Run as me to execute the job again. Once the Status changes to Success , click on the job name. Click on the History tab. Click on the latest run. In the output, a message is displayed to notify you that a new experiment was created. Click Experiments in the left-side menu. Click on the newly created experiment. Information about the experiment and a list of runs is displayed. The parameters and metrics for the run are displayed. Click on the run. A more detailed view of the run is displayed. Notice that in addition to the parameters and metrics, a run can have more information like notes, tags, and artifacts. Click Files in the left-side menu. Navigate to add.py and click Open in Session . Add the following code to create a new metric called count and a new artifact, the result.txt file: import sys import mlflow mlflow . set_experiment ( \"Add It Up\" ) mlflow . start_run () params = [ int ( number ) for number in sys . argv [ 1 :]] count = len ( params ) total = sum ( params ) with open ( \"result.txt\" , \"w\" ) as output_file : output_file . write ( f \"Input: { params } ,\" ) output_file . write ( f \"Count: { count } ,\" ) output_file . write ( f \"Sum: { total } \\n \" ) output_file . close () mlflow . log_param ( \"Input\" , params ) mlflow . log_metric ( \"Count\" , count ) mlflow . log_metric ( \"Sum\" , total ) mlflow . log_artifact ( \"result.txt\" ) print ( f \"The sum of the numbers is: { total } \" ) mlflow . end_run () Here is a brief description of the added lines: * count = len(params) - This is a new metric for tracking the number of numbers to sum. with open(\"result.txt\", \"w\") ... - This line and the following four lines create a file (artifact) with the results of the run. mlflow.log_metric(\"Count\", count) - This line tracks the new count metric. mlflow.log_artifact(\"result.txt\") - This line tracks the new artifact. Save the file and click Project to return to the project. Click Jobs in the left-side menu and click Run as me on the job. Click Experiments in the left-side menu and click the experiment in the list. Notice, the list of runs includes the new Count metric. Click on the latest run. The run details includes the new metric too. Scroll down, if needed, and view the Artifacts. If the job recently completed, the Artifacts may not have updated. Wait a minute and refresh the page, if you do not see the result.txt file in the list of Artifacts.","title":"Creating an Experiment"},{"location":"experiment_tracking/index.html#change-the-input-and-compare-runs","text":"Click Jobs in the left-side menu and click the Add It Up job in the list. Click the Settings tab. Edit the Arguments to be something different, like 20 30 40 . Scroll down and click Update Job Click Run as me . Click Experiments in the left-side menu and click on the Add It Up experiment. Select last two runs by clicking the respective checkboxes. Click Compare . Compare the results of the two runs using the table at the top and the various plots.","title":"Change the Input and Compare Runs"},{"location":"experiment_tracking/index.html#commit-the-changes-to-git","text":"Now that you have modified the add.py code to track metrics, you will commit the changes to git so they won't be lost. Click Sessions in the left-side menu and click New Session . Enter a session name and click Start Session . Once the session has started, click the Terminal Access button. Enter git status and press Enter . You will see add.py has been modified. Add the file to git and commit the changes. git add code/python/add.py git commit -m \"Added tracking to add.py\" Your file changes have been committed. Normally, you would push the changes back to a remote repository. However, in this case, you do not have permission to commit to the remote repository. End of Exercise","title":"Commit the Changes to Git"},{"location":"introduction/index.html","text":"Introduction to CML on CDP This exercise will introduce the Cloudera Data Platform (CDP) and Cloudera Machine Learning (CML). In this exercise, you will: login to the Cloudera Data Platform exercise environment, view the Cloudera Machine Learning workspace, create a new CML project, create a new session, and delete a CML project. Login to Cloudera Data Platform Open the Log In page for the CDP Public Cloud environment Enter the username and password provided by your instructor. Click Log In . The Cloudera Data Platform page is displayed. Some of the additional features of the platform will be explored in other exercises. For now, click Machine Learning . Overview of CML Workspace The list of Machine Learning workspaces is displayed. In this case, one workspace is provisioned. Click on cml-on-cdp . The Projects page is displayed. The Projects page lists all of the projects in the workspace. (Your screen will vary from the screen shown below.) The workspace has several helpful features. Select Learning Hub from the workspace menu. View the Learning Hub content. The Learning Hub is a great resource to learn about the new features in CML and read blog posts, research reports, and documentation. Select Runtime Catalog . The Runtime Catalog shows a list of available runtimes and allows new runtimes to be added. Select AMPs . The AMPs page shows a list of availble Applied ML Prototypes (AMPs). Select User Settings . Select the Outbound SSH Key tab. The Outbound SSH Key is used for connecting to external resources, for example Github. Applications, Jobs, Models, Experiments, and Sessions will be covered later. Project and Session Overview Create a New Project Select Projects from the workspace menu. Click the New Project button. For Project Name , enter Student # - First Project , where # is your student number. Leave the following items set to their default values: Description : Empty Project Visibility : Private Initial Setup : Template - Python Runtime setup : Basic Select Python 3.9 as the runtime Kernel . Click Create Project . Create a New Session After the new project is created, the project overview page is displayed and the left-hand menu displays project items instead of workspace items. Next, create a session to interact with your project. Click New Session . Enter First Session for the Session Name . Select JupyterLab as the Editor . Click Start Session . Close the Connection Code Snippet dialog. JupyterLab in CML is displayed. Double-click the analysis.ipynb notebook to open it. Note This exercise is just an introduction to creating a session that uses JupyterLabs. If you are are familar with JupyterLabs and would like to work though the analysis.ipynb notebook, be sure to uncomment %pip install -r requirements.txt in the first cell. Click \u2190 Project to leave the session and return to the project. Select Sessions . The list of sessions is displayed. Your session should be listed and show a status of Running . Delete a Project Select Project Settings. Select the Delete Project tab. Click Delete Project . Click OK to confirm. End of Exercise","title":"Introduction to CML on CDP"},{"location":"introduction/index.html#introduction-to-cml-on-cdp","text":"This exercise will introduce the Cloudera Data Platform (CDP) and Cloudera Machine Learning (CML). In this exercise, you will: login to the Cloudera Data Platform exercise environment, view the Cloudera Machine Learning workspace, create a new CML project, create a new session, and delete a CML project.","title":"Introduction to CML on CDP"},{"location":"introduction/index.html#login-to-cloudera-data-platform","text":"Open the Log In page for the CDP Public Cloud environment Enter the username and password provided by your instructor. Click Log In . The Cloudera Data Platform page is displayed. Some of the additional features of the platform will be explored in other exercises. For now, click Machine Learning .","title":"Login to Cloudera Data Platform"},{"location":"introduction/index.html#overview-of-cml-workspace","text":"The list of Machine Learning workspaces is displayed. In this case, one workspace is provisioned. Click on cml-on-cdp . The Projects page is displayed. The Projects page lists all of the projects in the workspace. (Your screen will vary from the screen shown below.) The workspace has several helpful features. Select Learning Hub from the workspace menu. View the Learning Hub content. The Learning Hub is a great resource to learn about the new features in CML and read blog posts, research reports, and documentation. Select Runtime Catalog . The Runtime Catalog shows a list of available runtimes and allows new runtimes to be added. Select AMPs . The AMPs page shows a list of availble Applied ML Prototypes (AMPs). Select User Settings . Select the Outbound SSH Key tab. The Outbound SSH Key is used for connecting to external resources, for example Github. Applications, Jobs, Models, Experiments, and Sessions will be covered later.","title":"Overview of CML Workspace"},{"location":"introduction/index.html#project-and-session-overview","text":"","title":"Project and Session Overview"},{"location":"markdown_examples/index.html","text":"Markdown Examples This is a list of various things you might need to do in an exercise. Some C++ with Callouts See important details below #ifndef _My_Parser_h_ 1 #define _My_Parser_h_ #include \"MyFetch.h\" 2 class My_Parser { 3 public : // Construction/Destruction My_Parser (); 3 virtual ~ My_Parser () = 0 ; virtual int parse ( MyFetch & fetcher ) = 0 ; }; #endif Commonly Used Tools for Data Ingestion The tool used for ingest is often dictated by the source of the data The hdfs dfs command is best for static documents Sqoop is used to exchange data with a database server Flume can collect data as it is generated from various sources Compact Table Style If the developer is having trouble fitting the table on the page, there is a compact option Method Description GET :material-check: Fetch resource PUT :material-check-all: Update resource DELETE :material-close: Delete resource Downloading the Course Materials Log in using https://education.cloudera.com If necessary, use the Register Now link on the right to create an account If you have forgotten your password, use the Reset Password link Scroll down to find this course If necessary, click My Learning under the photo You may also want to use the Current filter Select the course title Click the Resources tab Click a file to download it An Example Java Class This is a code sample It is a very simple Java class public static void main ( String [] args ) { int x = 5 ; int y = 7 ; System . out . println ( \"x + y = \" + x + y ); } Loading a Database Table into a Cluster Sqoop has built-in support for importing data into Hive and Impala Add the --hive-import option to your Sqoop command Creates the table in the Hive metastore Imports data from the RDBMS to the tables directory in HDFS $ sqoop import \\ --connect jdbc:mysql://localhost/loudacre \\ --username training \\ --password training \\ --fields-terminated-by '\\t' \\ --table employees \\ --hive-import Non-Standard Technical Elements Things that end up bold If you need to refer to Edit . If you need to tell a user to enter some userinput Things That End up Italics While on the subject of math To stay hydrated, I drink plenty of H 2 O Also if you\u2019re feelin\u2019 the power, can raise 5 4 Use Unicode for Greek letters (for example, \u03b1 is \u03b1)","title":"Markdown Examples"},{"location":"markdown_examples/index.html#markdown-examples","text":"This is a list of various things you might need to do in an exercise.","title":"Markdown Examples"},{"location":"markdown_examples/index.html#some-c-with-callouts","text":"See important details below #ifndef _My_Parser_h_ 1 #define _My_Parser_h_ #include \"MyFetch.h\" 2 class My_Parser { 3 public : // Construction/Destruction My_Parser (); 3 virtual ~ My_Parser () = 0 ; virtual int parse ( MyFetch & fetcher ) = 0 ; }; #endif","title":"Some C++ with Callouts"},{"location":"markdown_examples/index.html#commonly-used-tools-for-data-ingestion","text":"The tool used for ingest is often dictated by the source of the data The hdfs dfs command is best for static documents Sqoop is used to exchange data with a database server Flume can collect data as it is generated from various sources","title":"Commonly Used Tools for Data Ingestion"},{"location":"markdown_examples/index.html#compact-table-style","text":"If the developer is having trouble fitting the table on the page, there is a compact option Method Description GET :material-check: Fetch resource PUT :material-check-all: Update resource DELETE :material-close: Delete resource","title":"Compact Table Style"},{"location":"markdown_examples/index.html#downloading-the-course-materials","text":"Log in using https://education.cloudera.com If necessary, use the Register Now link on the right to create an account If you have forgotten your password, use the Reset Password link Scroll down to find this course If necessary, click My Learning under the photo You may also want to use the Current filter Select the course title Click the Resources tab Click a file to download it","title":"Downloading the Course Materials"},{"location":"markdown_examples/index.html#an-example-java-class","text":"This is a code sample It is a very simple Java class public static void main ( String [] args ) { int x = 5 ; int y = 7 ; System . out . println ( \"x + y = \" + x + y ); }","title":"An Example Java Class"},{"location":"markdown_examples/index.html#loading-a-database-table-into-a-cluster","text":"Sqoop has built-in support for importing data into Hive and Impala Add the --hive-import option to your Sqoop command Creates the table in the Hive metastore Imports data from the RDBMS to the tables directory in HDFS $ sqoop import \\ --connect jdbc:mysql://localhost/loudacre \\ --username training \\ --password training \\ --fields-terminated-by '\\t' \\ --table employees \\ --hive-import","title":"Loading a Database Table into a Cluster"},{"location":"markdown_examples/index.html#non-standard-technical-elements","text":"Things that end up bold If you need to refer to Edit . If you need to tell a user to enter some userinput Things That End up Italics While on the subject of math To stay hydrated, I drink plenty of H 2 O Also if you\u2019re feelin\u2019 the power, can raise 5 4 Use Unicode for Greek letters (for example, \u03b1 is \u03b1)","title":"Non-Standard Technical Elements"},{"location":"model_monitoring/index.html","text":"Continuous Model Monitoring with Evidently Monitoring a deployed machine learning model is critical to ensuring model performance over time. CML's model metrics combined with Evidently , an open source tool for evaluating, testing, and monitoring machine learning models, is a powerful tool to combat model drift. In this exercise, you will: Start the Continuous Model Monitoring AMP Launch the Price Regressor Monitoring dashboard Explore drift and variations in the model performance Identify the file that creates the Evidently dashboard Experiment with Evidently reports Warning The initial deployment of the AMP takes approximately 30 minutes. Start the Continuous Model Monitoring AMP Click AMPs Click Continuous Model Monitoring AMP Click Configure Project Set Dev Mode to True Warn You must select Python 3.7 or the AMP will fail. Select Python 3.7 Click Launch Project Launch the Price Regressor Monitoring dashboard Click Applications in the left-side menu Click the application \u22ee menu Click Application Details Click Settings Change Kernel to Python 3.7 Click Update Application button Click Applications in the left-side menu Click the application \u22ee menu, and select Restart Application Click Price Regressor Tile Explore drift and variations in the model performance Explore the Dashboard Explore the three tabs at different dates Is data monotonously drifting? Is there Numerical Target Drift at any point? Are there any significant variations in the model performance? Identify the file that creates the Evidently dashboard Project Structure 18. \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 .project-metadata.yaml # declarative specification for AMP logic \u251c\u2500\u2500 apps \u2502 \u251c\u2500\u2500 reports # folder to collect monitoring reports \u2502 \u2514\u2500\u2500 app.py # Flask app to serve monitoring reports \u251c\u2500\u2500 cdsw-build.sh # build script for model endpoint \u251c\u2500\u2500 data # directory to hold raw and working data artifacts \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 install_dependencies.py # commands to install python package dependencies \u2502 \u251c\u2500\u2500 predict.py # inference script that utilizes cdsw.model_metrics \u2502 \u251c\u2500\u2500 prepare_data.py # splits raw data into training and production sets \u2502 \u251c\u2500\u2500 simulate.py # script that runs simulated production logic \u2502 \u2514\u2500\u2500 train.py # build and train an sklearn pipeline for regression \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api.py # utility class for working with CML APIv2 \u251c\u2500\u2500 inference.py # utility class for concurrent model requests \u251c\u2500\u2500 simulation.py # utility class for simulation logic \u2514\u2500\u2500 utils.py # various utility functions The Continuous Model Monitoring AMP project structure is above. Select Models from the project menu, you see the Price Regressor model is deployed. Click on the Price Regressor model. Explore the model's properties. What file is the model based on? Which function is called? Click the Test button to test the model. View the results. Click Files in the project menu. Navigate to the scripts/predict.py file. Click the Open in Workbench button. Go to line 87 and examine the cdsw.track_metric call that is used to store the prediction metrics in the CML PostgreSQL metrics database. Note Your CML workspace must have the Enable Model Metrics option selected when it is provisioned in order to track metrics. The src\\simulation.py file is the main simulation routine and mimics a production monitoring use case. This simulation assumes the Price Regressor model has already been deployed, and accepts that model name as input. The .run_simulation() method operates the main logic of this class. Namely, it: Scores all training data against the deployed model so we can query metrics from the Model Metrics database for evaluation Initializes a simulation clock, which is just a list of date ranges from the prod_df to iterate over. These batches mimic the cadence upon which new data \"arrives\" in a production setting. For each simulation clock date_range, we: Query the prod_df for newly listed records and score them using deployed model Query the prod_df for newly sold records and add ground truths to metric store Query the metric store for those newly sold records and generate new Evidently report Redeploy the hosted Application to surface the new monitoring report Navigate to src/simulation.py and open the file in the Workbench Editor. Examine the query_model_metrics method on line 340. This method reads the metrics stored in the CML model metric database. Examine the build_evidently_reports method on line 404. This method builds the reports and saves them as HTML to be served by the Price Regressor Monitoring Dashboard application. Go to line 186 and examine how the simulation deploys the updated Price Regressor Monitoring Dashboard application upon simulation completion. This is an example of using the CML API to deploy an application. More details can be seen in the src/api.py file. End of Exercise","title":"Continuous Model Monitoring with Evidently AI"},{"location":"model_monitoring/index.html#continuous-model-monitoring-with-evidently","text":"Monitoring a deployed machine learning model is critical to ensuring model performance over time. CML's model metrics combined with Evidently , an open source tool for evaluating, testing, and monitoring machine learning models, is a powerful tool to combat model drift. In this exercise, you will: Start the Continuous Model Monitoring AMP Launch the Price Regressor Monitoring dashboard Explore drift and variations in the model performance Identify the file that creates the Evidently dashboard Experiment with Evidently reports Warning The initial deployment of the AMP takes approximately 30 minutes.","title":"Continuous Model Monitoring with Evidently"},{"location":"model_monitoring/index.html#start-the-continuous-model-monitoring-amp","text":"Click AMPs Click Continuous Model Monitoring AMP Click Configure Project Set Dev Mode to True Warn You must select Python 3.7 or the AMP will fail. Select Python 3.7 Click Launch Project","title":"Start the Continuous Model Monitoring AMP"},{"location":"model_monitoring/index.html#launch-the-price-regressor-monitoring-dashboard","text":"Click Applications in the left-side menu Click the application \u22ee menu Click Application Details Click Settings Change Kernel to Python 3.7 Click Update Application button Click Applications in the left-side menu Click the application \u22ee menu, and select Restart Application Click Price Regressor Tile","title":"Launch the Price Regressor Monitoring dashboard"},{"location":"model_monitoring/index.html#explore-drift-and-variations-in-the-model-performance","text":"Explore the Dashboard Explore the three tabs at different dates Is data monotonously drifting? Is there Numerical Target Drift at any point? Are there any significant variations in the model performance?","title":"Explore drift and variations in the model performance"},{"location":"model_monitoring/index.html#identify-the-file-that-creates-the-evidently-dashboard","text":"","title":"Identify the file that creates the Evidently dashboard"},{"location":"model_monitoring/code/index.html","text":"Evidently tutorial illustrating Data Drift and Categorical Target Drift (Multiclass) using the Iris dataset","title":"Index"},{"location":"streamlit/index.html","text":"Streamlit on CML \" Streamlit turns data scripts into shareable web apps in minutes.\" Streamlit is a web application framework that allows data scientists to quickly build web applications to share data and analytics. In this exercise, you will use an AMP (Applied ML Prototype) to deploy a simple Streamlit application using CML. You will learn how to deploy an AMP and how applications work in CML. Go to CML Workspace Begin the exercise by nagivating to your workspace. Click Machine Learning . Select the workspace. Create a New Project Using the Streamlit AMP Creating a project from an AMP is easy. Click AMPs in the workspace menu. You will see a catalog of AMPs. New AMPs are being added all of the time. In addition to AMPs provided by Cloudera, you can create your own AMPs and your own AMP catalog . Click the Streamlit tile. A dialog will popup that describes the AMP. The dialog also contains a link to the Github repository for the AMP. Viewing an AMP's repository is a good way to learn more about the AMP and how AMPs are created, in general. Click View on Github . Click the project-metadata.yaml file. In the project-metadata.yaml file, you can see the steps that are used to deploy the AMP. In this case, you can see that AMP runs a session to install the Python dependecies and then starts a new application called Streamlit App. name : Streamlit description : Run a Streamlit app inside CML. author : Cloudera Inc. specification_version : 1.0 prototype_version : 2.0 date : \"2023-03-24\" runtimes : - editor : Workbench kernel : Python 3.9 edition : Standard tasks : - type : run_session name : Install Dependencies script : cml/install_dependencies.py kernel : python3 cpu : 1 memory : 2 - type : start_application name : Streamlit App subdomain : streamlit script : cml/launch_app.py short_summary : Start Streamlit application environment_variables : TASK_TYPE : START_APPLICATION Close the Github tab and return to the AMP dialog. Click the Configure Project button. Disable Spark, if enabled. Click the Launch Project button. The AMP will begin building. You will see the two steps that were in the project-metadata.yaml file execute. Wait for AMP to complete. It will take approximately five minutes for the AMP to complete both steps. Explore the Streamlit Project Now that the AMP has completed creating a new project, you can view the contents of the project to see what was created. Click Overview in the project menu. The project overview shows any models or jobs that were created, the files contained in the project, and the contents of the README.md markdown file which describe the project. At the top of the overview page is the message regarding the project creation status which contains a useful link to the return to the project creation status page. Click View status page . Click View details for Step 2 - Start Streamlit Application. A new tab will open with an overview of the application deployment. The overview provides a link to the application, the script that created the application, and when the application was last started. Click the Logs tab. View the log output. Logs shows the output from the application. In this case, you can see the command that was used to start the application: !streamlit run app.py --server.port $CDSW_APP_PORT --server.address 127.0.0.1 Note CML applications are long-running web applications. Unlike sessions, applications do not timeout from inactivity. The application must use either CDSW_APP_PORT or CDSW_READONLY_PORT as its server port. Click Files in the project menu. Click the CML folder. Click launch_app.py . This is the file that was used to start the application. If you look back at the project-metadata.yaml file, you will see where it was specified in the start_application task. Click Sessions in the project menu. This is a list of all of the project sessions. Here, you can see the session used by the AMP to install the Python dependencies. Click Install Dependencies in the list of sessions. Again, you see the output of the session. Similarly, you can view the application created by the AMP. Click Applications in the project menu. Click Streamlit App . This opens a new tab and displays the application. The URL can be shared and viewed by others. By default, authentication for applications is enforced on all ports and users cannot create public applications. If desired, the Admin user can allow users to create public applications that can be accessed by unauthenticated users. Therefore, users will typically have to sign into the CDP environment before opening an application's URL. View Streamlit App. Modify the Application Return to the Streamlit project. Click Files in the project menu. Compare the app.py code to the output of the app itself. Click the Open in Workbench button. Change some of the markdown content and reload the app. Did you need to rebuild the app? Why is the first markdown block statement in a st.markdown call and not the second one? Study the jointplot seaborn function: https://seaborn.pydata.org/generated/seaborn.jointplot.html Change the type of the jointplot . Delete the Project Click Project Settings Click Delete Project tab Click Delete Project button Click OK to confirm Bonus If you finish early, explore other AMPs in the AMP catalog. End of Exercise Solution","title":"Streamlit on CML"},{"location":"streamlit/index.html#streamlit-on-cml","text":"\" Streamlit turns data scripts into shareable web apps in minutes.\" Streamlit is a web application framework that allows data scientists to quickly build web applications to share data and analytics. In this exercise, you will use an AMP (Applied ML Prototype) to deploy a simple Streamlit application using CML. You will learn how to deploy an AMP and how applications work in CML.","title":"Streamlit on CML"},{"location":"streamlit/index.html#go-to-cml-workspace","text":"Begin the exercise by nagivating to your workspace. Click Machine Learning . Select the workspace.","title":"Go to CML Workspace"},{"location":"streamlit/index.html#create-a-new-project-using-the-streamlit-amp","text":"Creating a project from an AMP is easy. Click AMPs in the workspace menu. You will see a catalog of AMPs. New AMPs are being added all of the time. In addition to AMPs provided by Cloudera, you can create your own AMPs and your own AMP catalog . Click the Streamlit tile. A dialog will popup that describes the AMP. The dialog also contains a link to the Github repository for the AMP. Viewing an AMP's repository is a good way to learn more about the AMP and how AMPs are created, in general. Click View on Github . Click the project-metadata.yaml file. In the project-metadata.yaml file, you can see the steps that are used to deploy the AMP. In this case, you can see that AMP runs a session to install the Python dependecies and then starts a new application called Streamlit App. name : Streamlit description : Run a Streamlit app inside CML. author : Cloudera Inc. specification_version : 1.0 prototype_version : 2.0 date : \"2023-03-24\" runtimes : - editor : Workbench kernel : Python 3.9 edition : Standard tasks : - type : run_session name : Install Dependencies script : cml/install_dependencies.py kernel : python3 cpu : 1 memory : 2 - type : start_application name : Streamlit App subdomain : streamlit script : cml/launch_app.py short_summary : Start Streamlit application environment_variables : TASK_TYPE : START_APPLICATION Close the Github tab and return to the AMP dialog. Click the Configure Project button. Disable Spark, if enabled. Click the Launch Project button. The AMP will begin building. You will see the two steps that were in the project-metadata.yaml file execute. Wait for AMP to complete. It will take approximately five minutes for the AMP to complete both steps.","title":"Create a New Project Using the Streamlit AMP"},{"location":"streamlit/index.html#explore-the-streamlit-project","text":"Now that the AMP has completed creating a new project, you can view the contents of the project to see what was created. Click Overview in the project menu. The project overview shows any models or jobs that were created, the files contained in the project, and the contents of the README.md markdown file which describe the project. At the top of the overview page is the message regarding the project creation status which contains a useful link to the return to the project creation status page. Click View status page . Click View details for Step 2 - Start Streamlit Application. A new tab will open with an overview of the application deployment. The overview provides a link to the application, the script that created the application, and when the application was last started. Click the Logs tab. View the log output. Logs shows the output from the application. In this case, you can see the command that was used to start the application: !streamlit run app.py --server.port $CDSW_APP_PORT --server.address 127.0.0.1 Note CML applications are long-running web applications. Unlike sessions, applications do not timeout from inactivity. The application must use either CDSW_APP_PORT or CDSW_READONLY_PORT as its server port. Click Files in the project menu. Click the CML folder. Click launch_app.py . This is the file that was used to start the application. If you look back at the project-metadata.yaml file, you will see where it was specified in the start_application task. Click Sessions in the project menu. This is a list of all of the project sessions. Here, you can see the session used by the AMP to install the Python dependencies. Click Install Dependencies in the list of sessions. Again, you see the output of the session. Similarly, you can view the application created by the AMP. Click Applications in the project menu. Click Streamlit App . This opens a new tab and displays the application. The URL can be shared and viewed by others. By default, authentication for applications is enforced on all ports and users cannot create public applications. If desired, the Admin user can allow users to create public applications that can be accessed by unauthenticated users. Therefore, users will typically have to sign into the CDP environment before opening an application's URL. View Streamlit App.","title":"Explore the Streamlit Project"},{"location":"streamlit/index.html#modify-the-application","text":"Return to the Streamlit project. Click Files in the project menu. Compare the app.py code to the output of the app itself. Click the Open in Workbench button. Change some of the markdown content and reload the app. Did you need to rebuild the app? Why is the first markdown block statement in a st.markdown call and not the second one? Study the jointplot seaborn function: https://seaborn.pydata.org/generated/seaborn.jointplot.html Change the type of the jointplot .","title":"Modify the Application"},{"location":"streamlit/index.html#delete-the-project","text":"Click Project Settings Click Delete Project tab Click Delete Project button Click OK to confirm","title":"Delete the Project"},{"location":"streamlit/index.html#bonus","text":"If you finish early, explore other AMPs in the AMP catalog. End of Exercise Solution","title":"Bonus"},{"location":"streamlit/solution.html","text":"Streamlit on CML - Solution You need to reload the page, but not the app. The app automatically picks up the changes. The first block of text includes markdown (hyperlinks, etc.) and therefore must be in an st.markdown call. The second block is just pure text and is treated like a string by the Python interpreter. One possible solution is the following: with sns . axes_style ( \"white\" ): st . pyplot ( sns . jointplot ( data = geyser , x = \"waiting\" , y = \"duration\" , hue = \"kind\" , kind = \"scatter\" ) ) Notice, kind was changed from kde to scatter .","title":"Streamlit on CML - Solution"},{"location":"streamlit/solution.html#streamlit-on-cml-solution","text":"You need to reload the page, but not the app. The app automatically picks up the changes. The first block of text includes markdown (hyperlinks, etc.) and therefore must be in an st.markdown call. The second block is just pure text and is treated like a string by the Python interpreter. One possible solution is the following: with sns . axes_style ( \"white\" ): st . pyplot ( sns . jointplot ( data = geyser , x = \"waiting\" , y = \"duration\" , hue = \"kind\" , kind = \"scatter\" ) ) Notice, kind was changed from kde to scatter .","title":"Streamlit on CML - Solution"},{"location":"workbench/index.html","text":"Using Workbench for Lecture and Exercises The next several lectures and exercises will be completed using the CML Workbench. The files for the lectures and exercises are provided as an AMP. In this brief exercise, you will create a new project based on the AMP. Navigate to your Cloudera Machine Learning workspace. Click the New Project button. Enter Student # for Project Name Select AMPs for the Initial Setup . Enter https://github.com/bshimel-cloudera/edu-cml-on-cdp into the Provide Git URL of your AMPs field. Click the Create Project button. Select Python 3.7 as the Kernel , if it is not already selected. Click the Launch Project button. Wait for AMP Setup to Complete. The AMP is installing the Python modules and exercise files. The process should take about 5 minutes to complete. Select Overview from the project menu on the left. Click the New Session button. Enter Student # for Session Name . Info The workbench exercises require Python 3.7 and Spark 2. Click the Start Session button. Close the Connection Code Snippet dialog. Navigate to exercises/code/python in the files on the left side of the editor. Click on cml.py to open the file in the editor. Highlight rows 1 - 20. Select Run / Run Lines from them menu. The output from the first twenty lines should be displayed on the right. End of Exercise","title":"Workbench Lecture and Exercises"},{"location":"workbench/index.html#using-workbench-for-lecture-and-exercises","text":"The next several lectures and exercises will be completed using the CML Workbench. The files for the lectures and exercises are provided as an AMP. In this brief exercise, you will create a new project based on the AMP. Navigate to your Cloudera Machine Learning workspace. Click the New Project button. Enter Student # for Project Name Select AMPs for the Initial Setup . Enter https://github.com/bshimel-cloudera/edu-cml-on-cdp into the Provide Git URL of your AMPs field. Click the Create Project button. Select Python 3.7 as the Kernel , if it is not already selected. Click the Launch Project button. Wait for AMP Setup to Complete. The AMP is installing the Python modules and exercise files. The process should take about 5 minutes to complete. Select Overview from the project menu on the left. Click the New Session button. Enter Student # for Session Name . Info The workbench exercises require Python 3.7 and Spark 2. Click the Start Session button. Close the Connection Code Snippet dialog. Navigate to exercises/code/python in the files on the left side of the editor. Click on cml.py to open the file in the editor. Highlight rows 1 - 20. Select Run / Run Lines from them menu. The output from the first twenty lines should be displayed on the right. End of Exercise","title":"Using Workbench for Lecture and Exercises"}]}